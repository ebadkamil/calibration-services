{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import re\n",
    "import h5py\n",
    "from functools import partial\n",
    "from itertools import product\n",
    "\n",
    "from calibration import Display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wokflow\n",
    "\n",
    "* General set up (Tab 0):\n",
    "   \n",
    "   - Module Number: Specify channel/module numbers that you want to analyse. Must be comma separated.\n",
    "   - Pulse indices: Pulses that have x-rays. You can provide a range (start:stop:step) or comma separated (1,3,4 etc) or combination of range and individual pulses (1:10:2, 11, 14, 16 etc)\n",
    "   - Region of interest (ROI x) in x for each module: Only range based (start:stop)\n",
    "   - Region of interest (ROI y) in y for each module: Only range based (start:stop)\n",
    "   \n",
    "* Dark Run (Tab 1):\n",
    "    \n",
    "    - Dark Run Folder: (str) /gpfs/exfel/exp/MID/201931/proposal/raw/run_number\n",
    "    - Train ids: Train indices to take average over. Range based (start:stop). Deafult is all train (:)\n",
    "    - Process Darak button: To evalute average. This is done in parallel over modules. Doesn't block\n",
    "      further analysis. Once results (average image and histograms) are available it be displayed automatically.\n",
    "    - Using Pulses (slider) and module numbers (dropbox) on top one can visulaize data for each pulses or modules.\n",
    "\n",
    "* Data Visualization (Tab 2):\n",
    "    \n",
    "    - Run Folder: (str) /gpfs/exfel/exp/MID/201931/proposal/raw/run_number\n",
    "    - Train ids: Train indices. Range based (start:stop). Use cautiosly. It loads all train data in memory.\n",
    "    Also processing is parallized over modules, therefore maximum pickle size has to be respected.\n",
    "    \n",
    "    - Subtract Dark. Once dark average data is available you will be able to check on subtract dark.\n",
    "    - Load Run: Once things are set up, load the run.\n",
    "    \n",
    "    - Fitting Procedure:\n",
    "    \n",
    "        - First chose some reasonable peak threshold to filter out number of peaks\n",
    "        - Chose peak distance to remove very close peaks,\n",
    "        - Once peaks are chosen, one can click on button Fit Histogram to fit Gaussians. Number of peaks define the number of gaussian functions that will be used to fit the histogram. Generally fitting with 3-4 peaks give reasonable fitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    dark_run_folder='/gpfs/exfel/exp/MID/201931/p900091/raw/r0504',\n",
    "    run_folder='/gpfs/exfel/exp/MID/201931/p900091/raw/r0491',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widgets Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = Display(config=config)\n",
    "d.control_panel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis without GUI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from scipy.ndimage import gaussian_filter, gaussian_filter1d\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from calibration import DataProcessing, gauss_fit, eval_statistics\n",
    "from karabo_data import by_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "    def DataProcessing(module_number, path, *,\n",
    "                       train_index=None, pulse_ids=None,\n",
    "                       rois=None, operation=None,\n",
    "                       dark_run=None):\n",
    "                   \n",
    "           \"\"\" Process Data\n",
    "\n",
    "                    Parameters\n",
    "                    ----------\n",
    "                    module_number: int\n",
    "                        Channel number between 0, 15\n",
    "                    path: str\n",
    "                        Path to Run folder\n",
    "                    train_index: karabo_data (by_index)\n",
    "                        Default (all trains by_index[:])\n",
    "                    pulse_ids: str\n",
    "                        For eg. \":\" to select all pulses in a train\n",
    "                                \"start:stop:step\" to select indices with certain step size\n",
    "                                \"1,2,3\" comma separated pulse index to select specific pulses\n",
    "                                \"1,2,3, 5:10\" mix of above two\n",
    "                        Default: all pulses \":\"\n",
    "                    rois: karabo_data slice constructor by_index\n",
    "                        Select ROI of image data. For eg. by_index[..., 0:128, 0:64]\n",
    "                        See karabo_data method: `get_array`\n",
    "\n",
    "                    operation: function\n",
    "                        For eg. functools.partial(np.mean, axis=0) to take mean over trains\n",
    "                    dark_run: ndarray\n",
    "                        dark_data to subtract\n",
    "\n",
    "                    Return\n",
    "                    ------\n",
    "                    out: ndarray\n",
    "                        Shape:  operation -> (n_trains, n_pulses, ..., slow_scan, fast_scan)\n",
    "                    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_number = 15\n",
    "pulse_ids = \"1:250:2\"\n",
    "rois = by_index[:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dark Run average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "dark_run_folder = \"/gpfs/exfel/exp/MID/201931/p900091/raw/r0504\"\n",
    "\n",
    "dark_train_index = by_index[:]\n",
    "\n",
    "# mean over train index \n",
    "operation = partial(np.mean, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_average = DataProcessing(\n",
    "    module_number, \n",
    "    dark_run_folder,\n",
    "    train_index=dark_train_index,\n",
    "    pulse_ids=pulse_ids,\n",
    "    rois=rois,\n",
    "    operation=operation)\n",
    "\n",
    "print(dark_average.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write dark data to file that can be used later\n",
    "\n",
    "dark_run = {module_number:dark_average}\n",
    "\n",
    "with h5py.File(\"dark_run.h5\", \"w\") as f:\n",
    "    for modno, data in dark_run.items():\n",
    "        g = f.create_group(f\"entry_1/instrument/module_{modno}\")\n",
    "        g.create_dataset('data', data=dark_run[modno])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtract Dark from a run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_folder = \"/gpfs/exfel/exp/MID/201931/p900091/raw/r0491\"\n",
    "\n",
    "proc_train_index = by_index[250:280]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_subtracted = DataProcessing(\n",
    "    module_number, \n",
    "    run_folder,\n",
    "    train_index=proc_train_index,\n",
    "    pulse_ids=pulse_ids,\n",
    "    rois=rois,\n",
    "    dark_run=dark_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_trains, pulses, gain_bits, slow_scan, fast_scan\n",
    "print(f\"Shape of Dark Subtracted Data: {dark_subtracted.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_sub_run = {module_number:dark_subtracted}\n",
    "\n",
    "with h5py.File(\"dark_subtracted.h5\", \"w\") as f:\n",
    "    for modno, data in dark_sub_run.items():\n",
    "        g = f.create_group(f\"entry_1/instrument/module_{modno}\")\n",
    "        g.create_dataset('data', data=dark_sub_run[modno])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Layout({\n",
       "    'margin': {'b': 40, 'l': 10, 'r': 0, 't': 50}, 'template': '...', 'width': 450\n",
       "})"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Empty Plots\n",
    "\n",
    "import ipywidgets as widget\n",
    "\n",
    "image_widget = go.FigureWidget(data=go.Heatmap(showscale=False))\n",
    "\n",
    "trace = [go.Bar(name=\"Data\"), go.Scatter(mode='markers', name=\"Peaks\", marker=dict(size=10)), \n",
    "         go.Scatter(mode='lines', name=\"Scipy Fit\"),  go.Scatter(mode='lines+markers', name=\"Minuit Fit\")]\n",
    "hist_widget = go.FigureWidget(data=trace)\n",
    "\n",
    "image_widget.layout.update(margin=dict(l=0, b=40, t=50), width=450)\n",
    "hist_widget.layout.update(margin=dict(r=0, l=10, b=40, t=50), width=450)\n",
    "\n",
    "#display(widget.HBox([image_widget, hist_widget]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear plots\n",
    "hist_widget.data[1].x = []\n",
    "hist_widget.data[1].y = []\n",
    "hist_widget.data[2].x = []\n",
    "hist_widget.data[2].y = []\n",
    "\n",
    "\n",
    "# Pulse index to visualize\n",
    "pulse_id = 11\n",
    "\n",
    "# Mean image over trains (In this case 30 trains)\n",
    "image_widget.data[0].z = np.mean(dark_subtracted[:, pulse_id, 0, ...], axis=0)\n",
    "\n",
    "# Evaluate histogram\n",
    "bin_centers, bin_counts = eval_statistics(dark_subtracted[:, pulse_id, 0, ...], bins=500)\n",
    "\n",
    "# Update histogram image\n",
    "\n",
    "hist_widget.data[0].x = bin_centers\n",
    "hist_widget.data[0].y = bin_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting dark subtracted data\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Peak finding to estimate intial fit parameters\n",
    "    \n",
    "    * Apply Gaussian filter to smooth out the histogram\n",
    "    * Evaluate peaks for this filtered data\n",
    "        - Peaks parameters: height, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_threshold = 1000\n",
    "peak_distance = 10\n",
    "\n",
    "# Apply Gaussian filter\n",
    "filtered = gaussian_filter(bin_counts, 1.5)\n",
    "\n",
    "# Evaluate peaks\n",
    "peaks, _ = find_peaks(filtered,\n",
    "                      height=peak_threshold,\n",
    "                      distance=peak_distance)\n",
    "\n",
    "#Plot peaks and Gaussian filtered curve. \n",
    "\n",
    "hist_widget.data[1].x = bin_centers[peaks]\n",
    "hist_widget.data[1].y = filtered[peaks]\n",
    "\n",
    "hist_widget.data[2].x = bin_centers\n",
    "hist_widget.data[2].y = filtered\n",
    "\n",
    "\n",
    "print(f\"Number of peaks: {len(peaks)}\")\n",
    "print(f\"Peak positions, amplitudes: {list(zip(bin_centers[peaks], filtered[peaks]))}\")\n",
    "\n",
    "hist_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Gaussian\n",
    "\n",
    "    * Number of peaks will decide number of Gaussians to fit data\n",
    "    \n",
    "        def gauss_fit(xdata, ydata, params):\n",
    "            \"\"\"\n",
    "            Parameters\n",
    "            ----------\n",
    "            xdata: 1d array\n",
    "            ydata: 1d array\n",
    "            params: list\n",
    "                [A1, A2, A3, ..., S1, S2, S3, ..., P1, P2, P3...]\n",
    "                A: Amplitude of Gaussian\n",
    "                S: Width of Gaussian\n",
    "                P: Centre of Gaussian\n",
    "                \n",
    "            Return\n",
    "            ------\n",
    "            fit_data: 1d array\n",
    "            popt: fit params\n",
    "            perr: np.sqrt(np.diag(pcov))\n",
    "                standard error in fit params\n",
    "\n",
    "            \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct initial fit params list\n",
    "params = []\n",
    "\n",
    "params.extend(filtered[peaks])                         # Extend Amplitudes [A1, A2, A3, ...]\n",
    "params.extend(np.full((len(bin_centers[peaks])), 10))  # Extend Sigma [S1, S2, S3, ...]\n",
    "params.extend(bin_centers[peaks])                      # Extend Positions [P1, P2, P3, ...]\n",
    "\n",
    "try:\n",
    "    fit_data, popt, perr = gauss_fit(bin_centers, filtered, params)\n",
    "    # Plot\n",
    "    hist_widget.data[2].x = bin_centers\n",
    "    hist_widget.data[2].y = fit_data\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "hist_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Parallelized over modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_modules = [12, 13, 15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dark_average = partial(\n",
    "    DataProcessing,\n",
    "    path=dark_run_folder,\n",
    "    pulse_ids=\"1:250:2\",\n",
    "    train_index=by_index[:],\n",
    "    rois=by_index[..., :, :],\n",
    "    operation=partial(np.mean, axis=0))\n",
    "\n",
    "\n",
    "futures = OrderedDict()\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=len(list_modules)) as executor:\n",
    "    ret = executor.map(dark_average, list_modules)\n",
    "\n",
    "for data in ret:\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking IMINUIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iminuit import Minuit\n",
    "from iminuit import minimize\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 30, 2, 512, 128)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"dark_subtracted.h5\", \"r\") as f:\n",
    "    data_dark_sub = np.moveaxis(f[\"entry_1/instrument/module_15/data\"][:], 0, 1)\n",
    "\n",
    "\n",
    "print(data_dark_sub.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 600\n",
    "peak_threshold = 200\n",
    "peak_distance = 20\n",
    "\n",
    "centers, counts = eval_statistics(data_dark_sub[0, :, 0, ...], bins=bins)\n",
    "\n",
    "filtered = gaussian_filter(counts, 2.5)\n",
    "filtered = counts\n",
    "# Evaluate peaks\n",
    "peaks, _ = find_peaks(filtered,\n",
    "                      height=peak_threshold,\n",
    "                      distance=peak_distance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340675.73885246686\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92f189898b54f489f4185d1e74554ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'name': 'Data',\n",
       "              'type': 'bar',\n",
       "              'uid': '72f753bd-7f03-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def gaussian(x, *params):\n",
    "    num_gaussians = int(len(params) / 3)\n",
    "    A = params[:num_gaussians]\n",
    "    w = params[num_gaussians:2*num_gaussians]\n",
    "    c = params[2*num_gaussians:3*num_gaussians]\n",
    "    y = sum(\n",
    "        [A[i]*np.exp(-(x-c[i])**2./(w[i])) for i in range(num_gaussians)])\n",
    "    \n",
    "    return y\n",
    "\n",
    "def least_squares_np(xdata, ydata,  params):  \n",
    "    yvar = 0.01\n",
    "    y = gaussian(xdata, *params) \n",
    "    \n",
    "    return np.sum((ydata - y) ** 2 / yvar)\n",
    "\n",
    "\n",
    "least_sq = partial(\n",
    "    least_squares_np, centers, filtered)\n",
    "\n",
    "\n",
    "# Construct initial fit params list\n",
    "params = []\n",
    "\n",
    "params.extend(filtered[peaks])                          # Extend Amplitudes [A1, A2, A3, ...]\n",
    "params.extend(np.full((len(centers[peaks])), 100))      # Extend Sigma [S1, S2, S3, ...]\n",
    "params.extend(centers[peaks])                           # Extend Positions [P1, P2, P3, ...]\n",
    "\n",
    "bounds_minuit = [ (0, None) if i < 2*(len(params) // 3 ) else (None, None) \n",
    "                  for i in range(len(params)) ]\n",
    "bounds_scipy = [[ 0 if  i < 2*(len(params) // 3 ) else -np.inf \n",
    "                 for i in range(len(params))], \n",
    "                np.inf]\n",
    "\n",
    "popt = params\n",
    "try:\n",
    "    popt, _ = curve_fit(\n",
    "        gaussian, centers, filtered, p0=params, bounds=bounds_scipy)\n",
    "except Exception as ex:\n",
    "    pass\n",
    "\n",
    "m = Minuit.from_array_func(\n",
    "    least_sq, params, error=0.1, errordef=1, limit=tuple(bounds_minuit))\n",
    "\n",
    "m.migrad()\n",
    "\n",
    "hist_widget.data[0].x = centers\n",
    "hist_widget.data[0].y = counts\n",
    "\n",
    "hist_widget.data[1].x = centers[peaks]\n",
    "hist_widget.data[1].y = filtered[peaks]\n",
    "\n",
    "hist_widget.data[2].x = centers\n",
    "hist_widget.data[2].y = gaussian(centers, *popt)\n",
    "hist_widget.data[3].x = centers\n",
    "hist_widget.data[3].y = gaussian(centers, *m.np_values())\n",
    "\n",
    "\n",
    "print(m.fval / (len(filtered) - len(params) )  )\n",
    "hist_widget"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calibration-kernel",
   "language": "python",
   "name": "calibration-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
